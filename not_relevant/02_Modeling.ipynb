{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from termcolor import colored, cprint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help funcs for colored output\n",
    "def green(txt):\n",
    "    return f\"\\x1b[32m{txt}\\x1b[0m\"\n",
    "def red(txt):\n",
    "    return f\"\\x1b[31m{txt}\\x1b[0m\"\n",
    "def blue(txt):\n",
    "    return f\"\\x1b[36m{txt}\\x1b[0m\"\n",
    "def bold(txt):\n",
    "        return f\"\\x1b[1m{txt}\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get raw Data\n",
    "df = pd.read_csv(\"data/csv/house_data_training.csv\", sep=';') \n",
    "# remove unnamed column\n",
    "df = df.iloc[:, 1:]\n",
    "#Transform string to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df.tail()\n",
    "#Drop rows with NaN values\n",
    "\n",
    "maeList = []#for model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 01 we have identified 3 lines with missing values. Due to the size of the data set, we can use the listwise Deletion - method at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=15000, step=1)\n",
      "RangeIndex(start=0, stop=14997, step=1)\n"
     ]
    }
   ],
   "source": [
    "def dropMissingValues(df):\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "print(df.index)\n",
    "df = dropMissingValues(df)   \n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "In Task 01_Exploration various qualitative problems within the data were identified. In this chapter different approaches are implemented to solve these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving a copy of the original df\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Basic Regression Model\n",
    "> Creating a basic linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_train_test(X_train, X_test, y_train, y_test):\n",
    "    '''Function for building Basic Regression Model'''\n",
    "\n",
    "    # fit the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate the model\n",
    "    ypred = model.predict(X_test)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    mae = mean_absolute_error(y_test, ypred)\n",
    "    maeList.append(np.round(mae))\n",
    "    #print(f'{bold(\"Mean Absolute Error\")}: {blue(np.round(mae))}\\n')\n",
    "\n",
    "    print(bold(f'MAE_List expanded:'))\n",
    "    for i, m in enumerate(maeList):\n",
    "        if i+1 == len(maeList):\n",
    "            print(f'model_{bold(i)} - \"Mean Absolute Error:\" {blue(m)}\\nScore: {np.round(score, 4)}') \n",
    "        else:\n",
    "            print(f'model_{bold(i)} - \"Mean Absolute Error:\" {m}\\nScore: {np.round(score, 4)}')\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the baseline regression model with the data barely edited (only missing values filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6328488.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "def splitData(df, test_size = 0.2):\n",
    "    '''function for splitting the data from a given df into the given test_size proportions'''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # Select price as label and remove price_data from list\n",
    "    X, y = df.drop(columns=[\"price\"]), df[\"price\"]\n",
    "    # Transform Column to a numeric value\n",
    "    if 'date' in df:\n",
    "        X[[\"date\"]] = X[[\"date\"]].apply(pd.to_numeric)\n",
    "    # Dataframes in numpy-Arrays konvertieren\n",
    "    X,y  = np.array(X.values.tolist()), np.array(y.values.tolist())\n",
    "    #split Data and train the model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = splitData(df_raw, 0.2)\n",
    "model_0 = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Data Preprocessing\n",
    "\n",
    "> Detect Outliers by building 3 Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the detected outliers from out trainingData(X_train and y_train) \n",
    "#we want to transform the traingsData from np_arrays to dfs and reverse for better handling\n",
    "\n",
    "def np_to_df(numpy_arr, column_list):\n",
    "    df = pd.DataFrame(numpy_arr, columns=column_list)\n",
    "    return df\n",
    "\n",
    "def df_to_np(df):\n",
    "    np_arr = df.to_numpy()\n",
    "    return np_arr\n",
    "\n",
    "def calc_model_after_drop(df, outlier_index_list):\n",
    "    '''function creates and evaluates the model after deleting the outliers from TRAINING_DATA given from the parameter outlier_index_list'''\n",
    "    #create columnList to transform X_Train\n",
    "    column_list = df.columns.to_list().remove('price')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "\n",
    "    #transfrom train Data into df to drop the outliers\n",
    "    df_X_Train = np_to_df(X_train, column_list)\n",
    "    df_y_Train = np_to_df(y_train, ['price'])\n",
    "\n",
    "    #calculate max index --> we only want to delete the outliers below this threshold\n",
    "    maxIndex = df_X_Train.index.stop\n",
    "\n",
    "    for o in list(outlier_index_list):\n",
    "        #rint(type(o))\n",
    "        if o >= maxIndex:\n",
    "            outlier_index_list.remove(o) \n",
    "                 \n",
    "    #drop the outlierts from the dfs \n",
    "    df_X_Train = df_X_Train.drop(df_X_Train.index[outlier_index_list])\n",
    "    df_y_Train = df_y_Train.drop(df_y_Train.index[outlier_index_list])\n",
    "\n",
    "    #transfrom back trainigdata to np_arrays\n",
    "    X_train = df_to_np(df_X_Train)\n",
    "    y_train = df_to_np(df_y_Train)\n",
    "\n",
    "    #evaluate model after dropping Outliers from training data\n",
    "    model = reg_train_test(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task1 we detected multiple rows with the value 9999999 in its price column which can be considered as Noise values. With the following function we can drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get thte indexes for the detected price values which are way too high.\n",
    "def get99(df=df_raw):\n",
    "    list99 = df.index[df['price'] == 9999999.9].tolist()\n",
    "    list90 = df.index[df['price'] == 99999999.0].tolist()\n",
    "    list99_combined =  list(set(list99) | set(list90))\n",
    "    return list99_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Darf man 99.9 werte aus testdaten droppen??? ansonsten traingsdaten mit mittelwert zu ersetzten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6276384.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Create and evaluate model after dropping the 99... values\n",
    "outlier_list_99 = get99(df_raw)\n",
    "model_1 = calc_model_after_drop(df_raw, outlier_list_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean99(df=df_raw):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> after dropping the 9999999 values we get a much lower mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop99_all(df, outlier_index_list):\n",
    "    return df.drop(df.index[outlier_index_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 1.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 1.0\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" \u001b[36m166876.0\u001b[0m\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "outlier_list_99 = get99(df_raw)\n",
    "df = drop99_all(df_raw,outlier_list_99)\n",
    "column_list = df.columns.to_list().remove('price')\n",
    "X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "model_0 = reg_train_test(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers\n",
    "In this figure we can see the distribution of the values for the different features. Some histograms show a skewed distribution. Sometimes you can immediately recognize Outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1:  based on descriptive statistics (Univariate outlier handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df=df_raw, std_multiply=3):\n",
    "    '''Univariate outlier detection based on descriptive statistics (three standard deviations)\n",
    "    can be useful to identify extreme outliers'''\n",
    "\n",
    "    feature_list=['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "        'sqft_lot', 'floors', 'dis_super', 'view', 'condition',\n",
    "        'grade', 'sqft_above', 'sqft_basement',\n",
    "        'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "    outliers_dict = {}#dict for storing outlierts for an outlier summary df\n",
    "    outlier_list_unique = []\n",
    "    print(bold(\"Potential Outliers:\"))\n",
    "    for feature in feature_list:\n",
    "        feature_data = df[feature]\n",
    "\n",
    "        df_feature = pd.concat([feature_data], axis=1)\n",
    "        df_feature[\"outlier\"] = 0\n",
    "\n",
    "        three_std=feature_data.std()*std_multiply\n",
    "        mean=feature_data.mean()\n",
    "\n",
    "        inlier_low=mean-three_std\n",
    "        inlier_high=mean+three_std\n",
    "\n",
    "        outlier_list = [] #list for storing indexes of outliers\n",
    "        for i, value in enumerate(feature_data):\n",
    "            if value < inlier_low or value > inlier_high:\n",
    "                outlier_list.append(i)\n",
    "                df_feature.iloc[i,1] = 1      \n",
    "\n",
    "        print(f'{bold(feature)} detected: {blue(len(outlier_list))}')\n",
    "        \n",
    "        if not len(outlier_list) == 0:\n",
    "            outliers_dict[str(feature)]=outlier_list\n",
    "            outlier_list_unique =  list(set(outlier_list_unique) | set(outlier_list))\n",
    "    \n",
    "    return outlier_list_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPotential Outliers:\u001b[0m\n",
      "\u001b[1mprice\u001b[0m detected: \u001b[36m421\u001b[0m\n",
      "\u001b[1mbedrooms\u001b[0m detected: \u001b[36m42\u001b[0m\n",
      "\u001b[1mbathrooms\u001b[0m detected: \u001b[36m26\u001b[0m\n",
      "\u001b[1msqft_living\u001b[0m detected: \u001b[36m55\u001b[0m\n",
      "\u001b[1msqft_lot\u001b[0m detected: \u001b[36m202\u001b[0m\n",
      "\u001b[1mfloors\u001b[0m detected: \u001b[36m0\u001b[0m\n",
      "\u001b[1mdis_super\u001b[0m detected: \u001b[36m0\u001b[0m\n",
      "\u001b[1mview\u001b[0m detected: \u001b[36m210\u001b[0m\n",
      "\u001b[1mcondition\u001b[0m detected: \u001b[36m0\u001b[0m\n",
      "\u001b[1mgrade\u001b[0m detected: \u001b[36m10\u001b[0m\n",
      "\u001b[1msqft_above\u001b[0m detected: \u001b[36m47\u001b[0m\n",
      "\u001b[1msqft_basement\u001b[0m detected: \u001b[36m42\u001b[0m\n",
      "\u001b[1msqft_living15\u001b[0m detected: \u001b[36m33\u001b[0m\n",
      "\u001b[1msqft_lot15\u001b[0m detected: \u001b[36m179\u001b[0m\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 166876.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6255913.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#get indexes of outlier Rows \n",
    "outlier_list_z_score = z_score(df_raw, 4)\n",
    "model_2 = calc_model_after_drop(df_raw, outlier_list_z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> principal components als methode für 2 dimensionale dargstellung geeignet (nicht sicher ob es in outlierhandling passt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: bsed on distances (Multivariate outlier handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_knn(df=df_raw, k=3, num_outliers=181):\n",
    "    #X_train needed\n",
    "    X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "\n",
    "    #normalize data to identify outliers\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X = scaler.fit_transform(X_train)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    outlier_indices=np.argpartition(distances[:,1],-num_outliers)[-num_outliers:]\n",
    " \n",
    "    return outlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 166876.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" 6255913.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m4\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6488984.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#get indexes of outlier Rows \n",
    "outlier_list_knn = outliers_knn(df_raw, 5, 1000) #181\n",
    "model_3 = calc_model_after_drop(df_raw, outlier_list_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: based on density clustering (Multivariate outlier handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_dbscan(df=df_raw, k=3, num_outliers=181, eps=0.42, min_samples=5):\n",
    "\n",
    "    #need distances\n",
    "    X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X = scaler.fit_transform(X_train)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "\n",
    "    inliers_list=[]\n",
    "    outliers_list=[]\n",
    "    index_upper=distances[:,1].size\n",
    "\n",
    "    for index in range (0,index_upper):\n",
    "        if clustering.labels_[index] == -1:\n",
    "            outliers_list.append(index)\n",
    "        else:\n",
    "            inliers_list.append(index)\n",
    "\n",
    "    return outliers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 166876.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" 6255913.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m4\u001b[0m - \"Mean Absolute Error:\" 6488984.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m5\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6727487.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#get indexes of outlier Rows \n",
    "outlier_list_dbscan = outliers_dbscan()\n",
    "model_4 = calc_model_after_drop(df_raw, outlier_list_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion Outliers Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 0: 6328488.0\n",
      "\u001b[32mmodel: 1: 6276384.0\u001b[0m\n",
      "\u001b[32mmodel: 2: 166876.0\u001b[0m\n",
      "\u001b[32mmodel: 3: 6255913.0\u001b[0m\n",
      "\u001b[31mmodel: 4: 6488984.0\u001b[0m\n",
      "\u001b[31mmodel: 5: 6727487.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "baseline = maeList[0]\n",
    "for i, model in enumerate(maeList):\n",
    "    if model > baseline:\n",
    "        print(red(f'model: {i}: {model}'))\n",
    "    elif model == baseline:\n",
    "        print(f'model: {i}: {model}')\n",
    "    else:\n",
    "        print(green(f'model: {i}: {model}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 999.9 Werte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split DataSet into data and target\n",
    "def getNoise(df, cv=5):\n",
    "\n",
    "    df_noise = df.drop(['date'], axis = 1)\n",
    "    x = df_noise.iloc[:,2:]\n",
    "    y = df_noise.iloc[:,1]\n",
    "\n",
    "    #Regressions Modelle\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    reg1 = GradientBoostingRegressor(random_state=1)\n",
    "    reg2 = BayesianRidge()\n",
    "    reg3 = DecisionTreeRegressor(max_depth=5, random_state=1)\n",
    "\n",
    "    reg1.fit(x,y)\n",
    "    reg2.fit(x,y)\n",
    "    reg3.fit(x,y)\n",
    "\n",
    "    ereg = VotingRegressor([('gb', reg1),('brr',  reg2),('dtr', reg3)])\n",
    "\n",
    "    ereg.fit(x, y)\n",
    "\n",
    "    y_pred=cross_val_predict(ereg,x,y, cv=cv)\n",
    "\n",
    "    xt = x[:20]\n",
    "    #real = y[:20]\n",
    "    pred1 = reg1.predict(xt)\n",
    "    pred2 = reg2.predict(xt)\n",
    "    pred3 = reg3.predict(xt)\n",
    "    pred4 = ereg.predict(xt)\n",
    "    y_pred20 = y_pred[:20]\n",
    "\n",
    "    mae=mean_absolute_error(y_pred,y)\n",
    "    noise_id=[]\n",
    "    for index,i in enumerate(y):\n",
    "        if y_pred[index] > i+mae*10:\n",
    "            noise_id.append(index)\n",
    "        elif y_pred[index] < i-mae*10:\n",
    "            noise_id.append(index)    \n",
    "\n",
    "    print(f\"Bei dem 10fachen MAE kann man bis zu {red(len(noise_id))} Noise Sätze finden\")\n",
    "    noise_index_list = df_noise.index.tolist()\n",
    "    return noise_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei dem 10fachen MAE kann man bis zu \u001b[31m427\u001b[0m Noise Sätze finden\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 166876.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" 6255913.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m4\u001b[0m - \"Mean Absolute Error:\" 6488984.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m5\u001b[0m - \"Mean Absolute Error:\" 6727487.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m6\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6255913.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_index_list = getNoise(df_raw)\n",
    "model = calc_model_after_drop(df_raw, outlier_list_z_score)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformed, standardized or normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reduction issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection / Instance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of features which show a higher corrleation with the label\n",
    "def getRelFeatures(df=df_raw):\n",
    "    corr =df.corr(method=\"spearman\")\n",
    "    rel_features =[]\n",
    "    corr_fig = corr[\"price\"]\n",
    "    ix = corr.sort_values('price', ascending=False).index\n",
    "    print(bold(\"Relevante Korrelationen:\"))\n",
    "    for i in ix:\n",
    "        if corr_fig[i]>= 0.3 or corr_fig[i]<=-0.3:\n",
    "            rel_features.append(i)\n",
    "            print(\"Corr\", bold(i),\"zum Label:\", green(round(corr_fig[i],3)))\n",
    "        else:\n",
    "            print(\"Corr\", bold(i),\"zum Label:\", red(round(corr_fig[i],3)))\n",
    "            \n",
    "    return rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, feature_list):\n",
    "    return df[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRelevante Korrelationen:\u001b[0m\n",
      "Corr \u001b[1mprice\u001b[0m zum Label: \u001b[32m1.0\u001b[0m\n",
      "Corr \u001b[1mgrade\u001b[0m zum Label: \u001b[32m0.64\u001b[0m\n",
      "Corr \u001b[1msqft_living\u001b[0m zum Label: \u001b[32m0.606\u001b[0m\n",
      "Corr \u001b[1msqft_living15\u001b[0m zum Label: \u001b[32m0.542\u001b[0m\n",
      "Corr \u001b[1msqft_above\u001b[0m zum Label: \u001b[32m0.516\u001b[0m\n",
      "Corr \u001b[1mbathrooms\u001b[0m zum Label: \u001b[32m0.493\u001b[0m\n",
      "Corr \u001b[1mlat\u001b[0m zum Label: \u001b[32m0.459\u001b[0m\n",
      "Corr \u001b[1mfloors\u001b[0m zum Label: \u001b[32m0.346\u001b[0m\n",
      "Corr \u001b[1mbedrooms\u001b[0m zum Label: \u001b[32m0.318\u001b[0m\n",
      "Corr \u001b[1mview\u001b[0m zum Label: \u001b[31m0.268\u001b[0m\n",
      "Corr \u001b[1msqft_basement\u001b[0m zum Label: \u001b[31m0.229\u001b[0m\n",
      "Corr \u001b[1myr_built\u001b[0m zum Label: \u001b[31m0.167\u001b[0m\n",
      "Corr \u001b[1mwaterfront\u001b[0m zum Label: \u001b[31m0.099\u001b[0m\n",
      "Corr \u001b[1myr_renovated\u001b[0m zum Label: \u001b[31m0.083\u001b[0m\n",
      "Corr \u001b[1mlong\u001b[0m zum Label: \u001b[31m0.079\u001b[0m\n",
      "Corr \u001b[1mzipcode\u001b[0m zum Label: \u001b[31m0.034\u001b[0m\n",
      "Corr \u001b[1msqft_lot\u001b[0m zum Label: \u001b[31m0.015\u001b[0m\n",
      "Corr \u001b[1msqft_lot15\u001b[0m zum Label: \u001b[31m0.009\u001b[0m\n",
      "Corr \u001b[1mid\u001b[0m zum Label: \u001b[31m0.008\u001b[0m\n",
      "Corr \u001b[1mdis_super\u001b[0m zum Label: \u001b[31m0.006\u001b[0m\n",
      "Corr \u001b[1mahf1\u001b[0m zum Label: \u001b[31m0.005\u001b[0m\n",
      "Corr \u001b[1mahf3\u001b[0m zum Label: \u001b[31m0.002\u001b[0m\n",
      "Corr \u001b[1mahf2\u001b[0m zum Label: \u001b[31m-0.005\u001b[0m\n",
      "Corr \u001b[1mcondition\u001b[0m zum Label: \u001b[31m-0.019\u001b[0m\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 166876.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" 6255913.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m4\u001b[0m - \"Mean Absolute Error:\" 6488984.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m5\u001b[0m - \"Mean Absolute Error:\" 6727487.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m6\u001b[0m - \"Mean Absolute Error:\" 6255913.0\n",
      "Score: 0.0\n",
      "model_\u001b[1m7\u001b[0m - \"Mean Absolute Error:\" \u001b[36m5751352.0\u001b[0m\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "rel_features = getRelFeatures()\n",
    "df = drop_features(df_raw, rel_features)\n",
    "column_list = df.columns.to_list().remove('price')\n",
    "X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "model = reg_train_test(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> hinweise aus der vorlesung die interessant sein könnten:\n",
    "* principal components\n",
    "* EFA – Determine number of factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Selection: Experiment with different regression algorithms, e.g. linear regression, polynomial regression, regression trees etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Hyper-parameter Tuning: Change the hyper-parameters of your algorithms (e.g.„degree“ in case of polynomial regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49d1cf207c92197553c1326cc52484d1ee2809997f5109c15474876a3e083b6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "954f5f24ad6d00adbc06a20695ec5fdb3f1a5fa43282de1749a6705a15dcbfc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

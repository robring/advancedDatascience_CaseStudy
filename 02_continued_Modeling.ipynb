{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import DBSCAN\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help funcs for colored output\n",
    "def green(txt):\n",
    "    return f\"\\x1b[32m{txt}\\x1b[0m\"\n",
    "def red(txt):\n",
    "    return f\"\\x1b[31m{txt}\\x1b[0m\"\n",
    "def blue(txt):\n",
    "    return f\"\\x1b[36m{txt}\\x1b[0m\"\n",
    "def bold(txt):\n",
    "        return f\"\\x1b[1m{txt}\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get raw Data\n",
    "df = pd.read_csv(\"data/csv/house_data_training.csv\", sep=';') \n",
    "# remove unnamed column\n",
    "df = df.iloc[:, 1:]\n",
    "#Transform string to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "maeList = []#for model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 01 we have identified 3 lines with missing values. Due to the size of the data set, we can use the listwise Deletion - method at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropMissingValues(df):\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    #df = df.set_index(\"id\")\n",
    "    return df\n",
    "\n",
    "df = dropMissingValues(df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Parameters from json-File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRE_PROCESSING': {'missing_vlaues': 'df.dropna()', 'outliers': '', 'noise': '', 'transformation': '', 'normalisierung': '', 'standartisierung': '', 'feature_selection': ''}, 'ALGO_SELECTION': {'key_x': 'function_x', 'key_y': ''}, 'HYPER_PARAMETER': {'key_x': 'function_x', 'key_y': ''}}\n"
     ]
    }
   ],
   "source": [
    "# JSON file\n",
    "f = open ('data/json/input.json', \"r\")\n",
    "  \n",
    "# Reading from file\n",
    "PARAMETERS = json.loads(f.read())\n",
    "print(PARAMETERS)\n",
    "  \n",
    "# # Iterating through the json\n",
    "# # list\n",
    "# for i in data['emp_details']:\n",
    "#     print(i)\n",
    "  \n",
    "# Closing file\n",
    "f.close()\n",
    "\n",
    "SPLIT_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "In Task 01_Exploration various qualitative problems within the data were identified. In this chapter different approaches are implemented to solve these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Basic Regression Model\n",
    "> Creating a basic linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_train_test(X_train, X_test, y_train, y_test):\n",
    "    '''Function for building Basic Regression Model'''\n",
    "\n",
    "    # fit the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate the model\n",
    "    ypred = model.predict(X_test)\n",
    "    score = model.score(X_test, y_test)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    mae = mean_absolute_error(y_test, ypred)\n",
    "    maeList.append(np.round(mae))\n",
    "    #print(f'{bold(\"Mean Absolute Error\")}: {blue(np.round(mae))}\\n')\n",
    "\n",
    "    print(bold(f'MAE_List expanded:'))\n",
    "    for i, m in enumerate(maeList):\n",
    "        if i+1 == len(maeList):\n",
    "            print(f'model_{bold(i)} - \"Mean Absolute Error:\" {blue(m)}\\nScore: {np.round(score, 4)}') \n",
    "        else:\n",
    "            print(f'model_{bold(i)} - \"Mean Absolute Error:\" {m}\\nScore: {np.round(score, 4)}')\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the baseline regression model with the data barely edited (only missing values filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6328488.0\u001b[0m\n",
      "Score: 0.0503\n"
     ]
    }
   ],
   "source": [
    "def splitData(df, test_size = 0.2, outlier_index_list = []):\n",
    "    '''function for splitting the data from a given df into the given test_size proportions'''\n",
    "    \n",
    "    # Select price as label and remove price_data from list\n",
    "    X, y = df.drop(columns=[\"price\"]), df[\"price\"]\n",
    "    # Transform Column to a numeric value\n",
    "    if 'date' in df:\n",
    "        X[[\"date\"]] = X[[\"date\"]].apply(pd.to_numeric)\n",
    "    # Dataframes in numpy-Arrays konvertieren\n",
    "    X,y  = np.array(X.values.tolist()), np.array(y.values.tolist())\n",
    "    #split Data and train the model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=1)\n",
    "\n",
    "    if not outlier_index_list == []:\n",
    "        #create columnList to transform X_Train\n",
    "        column_list = df.columns.to_list().remove('price')\n",
    "\n",
    "        #transfrom train Data into df to drop the outliers\n",
    "        df_X_Train = np_to_df(X_train, column_list)\n",
    "        df_y_Train = np_to_df(y_train, ['price'])\n",
    "\n",
    "        #calculate max index --> we only want to delete the outliers below this threshold\n",
    "        maxIndex = df_X_Train.index.stop\n",
    "\n",
    "        for o in list(outlier_index_list):\n",
    "            #rint(type(o))\n",
    "            if o >= maxIndex:\n",
    "                outlier_index_list.remove(o) \n",
    "                    \n",
    "        #drop the outlierts from the dfs \n",
    "        df_X_Train = df_X_Train.drop(df_X_Train.index[outlier_index_list])\n",
    "        df_y_Train = df_y_Train.drop(df_y_Train.index[outlier_index_list])\n",
    "\n",
    "        #transfrom back trainigdata to np_arrays\n",
    "        X_train = df_to_np(df_X_Train)\n",
    "        y_train = df_to_np(df_y_Train)\n",
    "        print(f'dropped {red(len(outlier_index_list))} rows')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = splitData(df, SPLIT_SIZE)\n",
    "model_0 = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Data Preprocessing\n",
    "\n",
    "> Detect Outliers by building 3 Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the detected outliers from out trainingData(X_train and y_train) \n",
    "#we want to transform the traingsData from np_arrays to dfs and reverse for better handling\n",
    "\n",
    "def np_to_df(numpy_arr, column_list):\n",
    "    df = pd.DataFrame(numpy_arr, columns=column_list)\n",
    "    return df\n",
    "\n",
    "def df_to_np(df):\n",
    "    np_arr = df.to_numpy()\n",
    "    return np_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task1 we detected multiple rows with the value 9999999 in its price column which can be considered as Noise values. With the following function we can drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get thte indexes for the detected price values which are way too high.\n",
    "def get99(df):\n",
    "    list99 = df.index[df['price'] == 9999999.9].tolist()\n",
    "    list90 = df.index[df['price'] == 99999999.0].tolist()\n",
    "    list99_combined =  list(set(list99) | set(list90))\n",
    "    return list99_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Darf man 99.9 werte aus testdaten droppen??? ansonsten traingsdaten mit mittelwert zu ersetzten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped \u001b[31m341\u001b[0m rows\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0502\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6276384.0\u001b[0m\n",
      "Score: 0.0502\n"
     ]
    }
   ],
   "source": [
    "#Create and evaluate model after dropping the 99... values\n",
    "outlier_list_99 = get99(df)\n",
    "X_train, X_test, y_train, y_test = splitData(df, SPLIT_SIZE , outlier_list_99)\n",
    "model_1 = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>dis_super</th>\n",
       "      <th>...</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>ahf1</th>\n",
       "      <th>ahf2</th>\n",
       "      <th>ahf3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>5700000905</td>\n",
       "      <td>2014-08-16</td>\n",
       "      <td>739000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2840</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>694.64</td>\n",
       "      <td>...</td>\n",
       "      <td>1913.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5817</td>\n",
       "      <td>-122.291</td>\n",
       "      <td>2200</td>\n",
       "      <td>5000</td>\n",
       "      <td>24.130536</td>\n",
       "      <td>112.985690</td>\n",
       "      <td>137.116226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>2623069106</td>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>710000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3830</td>\n",
       "      <td>68825</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1156.61</td>\n",
       "      <td>...</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.4574</td>\n",
       "      <td>-122.003</td>\n",
       "      <td>2410</td>\n",
       "      <td>68825</td>\n",
       "      <td>83.830652</td>\n",
       "      <td>90.383630</td>\n",
       "      <td>174.214282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>1530900290</td>\n",
       "      <td>2014-10-07</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2280</td>\n",
       "      <td>3710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1352.78</td>\n",
       "      <td>...</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98072</td>\n",
       "      <td>47.7350</td>\n",
       "      <td>-122.159</td>\n",
       "      <td>2030</td>\n",
       "      <td>3710</td>\n",
       "      <td>13.268193</td>\n",
       "      <td>117.766765</td>\n",
       "      <td>131.034957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>4310701600</td>\n",
       "      <td>2014-11-13</td>\n",
       "      <td>340000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1240</td>\n",
       "      <td>1115</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1245.41</td>\n",
       "      <td>...</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6985</td>\n",
       "      <td>-122.340</td>\n",
       "      <td>1410</td>\n",
       "      <td>1355</td>\n",
       "      <td>48.352194</td>\n",
       "      <td>102.297062</td>\n",
       "      <td>150.649256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>8079010230</td>\n",
       "      <td>2014-06-03</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2600</td>\n",
       "      <td>7210</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>874.81</td>\n",
       "      <td>...</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.151</td>\n",
       "      <td>2350</td>\n",
       "      <td>7225</td>\n",
       "      <td>12.626021</td>\n",
       "      <td>95.332639</td>\n",
       "      <td>107.958660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "14992  5700000905 2014-08-16  739000.0         5        2.5         2840   \n",
       "14993  2623069106 2015-02-19  710000.0         6        3.5         3830   \n",
       "14994  1530900290 2014-10-07  475000.0         3        2.5         2280   \n",
       "14995  4310701600 2014-11-13  340000.0         3        2.5         1240   \n",
       "14996  8079010230 2014-06-03  475000.0         3        2.5         2600   \n",
       "\n",
       "       sqft_lot  floors  waterfront  dis_super  ...  yr_built  yr_renovated  \\\n",
       "14992      5000     1.0         0.0     694.64  ...    1913.0             0   \n",
       "14993     68825     2.0         0.0    1156.61  ...    1995.0             0   \n",
       "14994      3710     1.0         0.0    1352.78  ...    1990.0             0   \n",
       "14995      1115     3.0         0.0    1245.41  ...    2003.0             0   \n",
       "14996      7210     2.0         0.0     874.81  ...    1989.0             0   \n",
       "\n",
       "       zipcode      lat     long  sqft_living15  sqft_lot15       ahf1  \\\n",
       "14992    98144  47.5817 -122.291           2200        5000  24.130536   \n",
       "14993    98027  47.4574 -122.003           2410       68825  83.830652   \n",
       "14994    98072  47.7350 -122.159           2030        3710  13.268193   \n",
       "14995    98103  47.6985 -122.340           1410        1355  48.352194   \n",
       "14996    98059  47.5123 -122.151           2350        7225  12.626021   \n",
       "\n",
       "             ahf2        ahf3  \n",
       "14992  112.985690  137.116226  \n",
       "14993   90.383630  174.214282  \n",
       "14994  117.766765  131.034957  \n",
       "14995  102.297062  150.649256  \n",
       "14996   95.332639  107.958660  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()\n",
    "#14992\t5700000905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean99(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> after dropping the 9999999 values we get a much lower mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop99_all(df, outlier_index_list):\n",
    "    return df.drop(df.index[outlier_index_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_list_99 = get99(df_raw)\n",
    "\n",
    "# df = drop99_all(df_raw,outlier_list_99)\n",
    "# column_list = df.columns.to_list().remove('price')\n",
    "# X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "# model_0 = reg_train_test(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers\n",
    "In this figure we can see the distribution of the values for the different features. Some histograms show a skewed distribution. Sometimes you can immediately recognize Outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1:  based on descriptive statistics (Univariate outlier handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df, std_multiply=3):\n",
    "    '''Univariate outlier detection based on descriptive statistics (three standard deviations)\n",
    "    can be useful to identify extreme outliers'''\n",
    "\n",
    "    feature_list=['price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
    "        'sqft_lot', 'floors', 'dis_super', 'view', 'condition',\n",
    "        'grade', 'sqft_above', 'sqft_basement',\n",
    "        'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "    outliers_dict = {}#dict for storing outlierts for an outlier summary df\n",
    "    outlier_list_unique = []\n",
    "    print(bold(\"Potential Outliers:\"))\n",
    "    for feature in feature_list:\n",
    "        feature_data = df[feature]\n",
    "\n",
    "        df_feature = pd.concat([feature_data], axis=1)\n",
    "        df_feature[\"outlier\"] = 0\n",
    "\n",
    "        three_std=feature_data.std()*std_multiply\n",
    "        mean=feature_data.mean()\n",
    "\n",
    "        inlier_low=mean-three_std\n",
    "        inlier_high=mean+three_std\n",
    "\n",
    "        outlier_list = [] #list for storing indexes of outliers\n",
    "        for i, value in enumerate(feature_data):\n",
    "            if value < inlier_low or value > inlier_high:\n",
    "                outlier_list.append(i)\n",
    "                df_feature.iloc[i,1] = 1      \n",
    "\n",
    "        print(f'{bold(feature)} detected: {blue(len(outlier_list))}')\n",
    "        \n",
    "        if not len(outlier_list) == 0:\n",
    "            outliers_dict[str(feature)]=outlier_list\n",
    "            outlier_list_unique =  list(set(outlier_list_unique) | set(outlier_list))\n",
    "    \n",
    "    return outlier_list_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPotential Outliers:\u001b[0m\n",
      "\u001b[1mprice\u001b[0m detected: \u001b[36m421\u001b[0m\n",
      "\u001b[1mbedrooms\u001b[0m detected: \u001b[36m42\u001b[0m\n",
      "\u001b[1mbathrooms\u001b[0m detected: \u001b[36m26\u001b[0m\n",
      "\u001b[1msqft_living\u001b[0m detected: \u001b[36m55\u001b[0m\n",
      "\u001b[1msqft_lot\u001b[0m detected: \u001b[36m202\u001b[0m\n",
      "\u001b[1mfloors\u001b[0m detected: \u001b[36m0\u001b[0m\n",
      "\u001b[1mdis_super\u001b[0m detected: \u001b[36m0\u001b[0m\n",
      "\u001b[1mview\u001b[0m detected: \u001b[36m210\u001b[0m\n",
      "\u001b[1mcondition\u001b[0m detected: \u001b[36m0\u001b[0m\n",
      "\u001b[1mgrade\u001b[0m detected: \u001b[36m10\u001b[0m\n",
      "\u001b[1msqft_above\u001b[0m detected: \u001b[36m47\u001b[0m\n",
      "\u001b[1msqft_basement\u001b[0m detected: \u001b[36m42\u001b[0m\n",
      "\u001b[1msqft_living15\u001b[0m detected: \u001b[36m33\u001b[0m\n",
      "\u001b[1msqft_lot15\u001b[0m detected: \u001b[36m179\u001b[0m\n",
      "dropped \u001b[31m806\u001b[0m rows\n"
     ]
    }
   ],
   "source": [
    "#get indexes of outlier Rows \n",
    "outlier_list_z_score = z_score(df, 4)\n",
    "model_2 = splitData(df, SPLIT_SIZE, outlier_list_z_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> principal components als methode für 2 dimensionale dargstellung geeignet (nicht sicher ob es in outlierhandling passt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: bsed on distances (Multivariate outlier handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_knn(df, k=3, num_outliers=181):\n",
    "    #X_train needed\n",
    "    X_train, X_test, y_train, y_test = splitData(df, SPLIT_SIZE)\n",
    "\n",
    "    #normalize data to identify outliers\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X = scaler.fit_transform(X_train)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    outlier_indices=np.argpartition(distances[:,1],-num_outliers)[-num_outliers:]\n",
    " \n",
    "    return outlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped \u001b[31m200\u001b[0m rows\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0507\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0507\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6346856.0\u001b[0m\n",
      "Score: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\AppData\\Local\\Temp/ipykernel_14076/439116319.py:14: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if not outlier_index_list == []:\n"
     ]
    }
   ],
   "source": [
    "#get indexes of outlier Rows \n",
    "outlier_list_knn = outliers_knn(df, 3, 200) #181\n",
    "X_train, X_test, y_train, y_test = splitData(df, SPLIT_SIZE, outlier_list_knn)\n",
    "model = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: based on density clustering (Multivariate outlier handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_dbscan(df, k=3, num_outliers=181, eps=0.42, min_samples=5):\n",
    "\n",
    "    #need distances\n",
    "    X_train, X_test, y_train, y_test = splitData(df, 0.2)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X = scaler.fit_transform(X_train)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "\n",
    "    inliers_list=[]\n",
    "    outliers_list=[]\n",
    "    index_upper=distances[:,1].size\n",
    "\n",
    "    for index in range (0,index_upper):\n",
    "        if clustering.labels_[index] == -1:\n",
    "            outliers_list.append(index)\n",
    "        else:\n",
    "            inliers_list.append(index)\n",
    "\n",
    "    return outliers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped \u001b[31m2762\u001b[0m rows\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0459\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0459\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 6346856.0\n",
      "Score: 0.0459\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6727487.0\u001b[0m\n",
      "Score: 0.0459\n"
     ]
    }
   ],
   "source": [
    "#get indexes of outlier Rows \n",
    "outlier_list_dbscan = outliers_dbscan(df)\n",
    "X_train, X_test, y_train, y_test = splitData(df, SPLIT_SIZE, outlier_list_dbscan)\n",
    "model = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion Outliers Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 0: 6328488.0\n",
      "\u001b[32mmodel: 1: 6276384.0\u001b[0m\n",
      "\u001b[31mmodel: 2: 6346856.0\u001b[0m\n",
      "\u001b[31mmodel: 3: 6727487.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "baseline = maeList[0]\n",
    "for i, model in enumerate(maeList):\n",
    "    if model > baseline:\n",
    "        print(red(f'model: {i}: {model}'))\n",
    "    elif model == baseline:\n",
    "        print(f'model: {i}: {model}')\n",
    "    else:\n",
    "        print(green(f'model: {i}: {model}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 999.9 Werte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split DataSet into data and target\n",
    "def getNoise(df, cv=5):\n",
    "\n",
    "    df_noise = df.drop(['date'], axis = 1)\n",
    "    x = df_noise.iloc[:,2:]\n",
    "    y = df_noise.iloc[:,1]\n",
    "\n",
    "    #Regressions Modelle\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    reg1 = GradientBoostingRegressor(random_state=1)\n",
    "    reg2 = BayesianRidge()\n",
    "    reg3 = DecisionTreeRegressor(max_depth=5, random_state=1)\n",
    "\n",
    "    reg1.fit(x,y)\n",
    "    reg2.fit(x,y)\n",
    "    reg3.fit(x,y)\n",
    "\n",
    "    ereg = VotingRegressor([('gb', reg1),('brr',  reg2),('dtr', reg3)])\n",
    "\n",
    "    ereg.fit(x, y)\n",
    "\n",
    "    y_pred=cross_val_predict(ereg,x,y, cv=cv)\n",
    "\n",
    "    xt = x[:20]\n",
    "    #real = y[:20]\n",
    "    pred1 = reg1.predict(xt)\n",
    "    pred2 = reg2.predict(xt)\n",
    "    pred3 = reg3.predict(xt)\n",
    "    pred4 = ereg.predict(xt)\n",
    "    y_pred20 = y_pred[:20]\n",
    "\n",
    "    mae=mean_absolute_error(y_pred,y)\n",
    "    noise_id=[]\n",
    "    for i, e in enumerate(y):\n",
    "        if y_pred[i] > e+mae*10:\n",
    "            noise_id.append(i)\n",
    "        elif y_pred[i] < e-mae*10:\n",
    "            noise_id.append(i)    \n",
    "\n",
    "    print(f\"Bei dem 10fachen MAE kann man bis zu {red(len(noise_id))} Noise Sätze finden\")\n",
    "    #noise_index_list = df_noise.index.tolist()\n",
    "    noise_index_list = df.iloc[noise_id,].index\n",
    "    noise_index_list = noise_index_list.to_list()\n",
    "    return noise_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bei dem 10fachen MAE kann man bis zu \u001b[31m427\u001b[0m Noise Sätze finden\n",
      "dropped \u001b[31m338\u001b[0m rows\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0503\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0503\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 6346856.0\n",
      "Score: 0.0503\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" 6727487.0\n",
      "Score: 0.0503\n",
      "model_\u001b[1m4\u001b[0m - \"Mean Absolute Error:\" \u001b[36m6261226.0\u001b[0m\n",
      "Score: 0.0503\n"
     ]
    }
   ],
   "source": [
    "outlier_list_noise = getNoise(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = splitData(df, SPLIT_SIZE, outlier_list_noise)\n",
    "model = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformed, standardized or normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reduction issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection / Instance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of features which show a higher corrleation with the label\n",
    "#TODO threshold for corr\n",
    "def getRelFeatures(df):\n",
    "    corr =df.corr(method=\"spearman\")\n",
    "    rel_features =[]\n",
    "    corr_fig = corr[\"price\"]\n",
    "    ix = corr.sort_values('price', ascending=False).index\n",
    "    print(bold(\"Relevante Korrelationen:\"))\n",
    "    for i in ix:\n",
    "        if corr_fig[i]>= 0.3 or corr_fig[i]<=-0.3:\n",
    "            rel_features.append(i)\n",
    "        #     print(\"Corr\", bold(i),\"zum Label:\", green(round(corr_fig[i],3)))\n",
    "        # else:\n",
    "        #     print(\"Corr\", bold(i),\"zum Label:\", red(round(corr_fig[i],3)))\n",
    "            \n",
    "    return rel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, feature_list):\n",
    "    try:\n",
    "        return df[feature_list]\n",
    "    except:\n",
    "        print(f'Error while trying to drop features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRelevante Korrelationen:\u001b[0m\n",
      "\u001b[1mMAE_List expanded:\u001b[0m\n",
      "model_\u001b[1m0\u001b[0m - \"Mean Absolute Error:\" 6328488.0\n",
      "Score: 0.0441\n",
      "model_\u001b[1m1\u001b[0m - \"Mean Absolute Error:\" 6276384.0\n",
      "Score: 0.0441\n",
      "model_\u001b[1m2\u001b[0m - \"Mean Absolute Error:\" 6346856.0\n",
      "Score: 0.0441\n",
      "model_\u001b[1m3\u001b[0m - \"Mean Absolute Error:\" 6727487.0\n",
      "Score: 0.0441\n",
      "model_\u001b[1m4\u001b[0m - \"Mean Absolute Error:\" 6261226.0\n",
      "Score: 0.0441\n",
      "model_\u001b[1m5\u001b[0m - \"Mean Absolute Error:\" \u001b[36m5751352.0\u001b[0m\n",
      "Score: 0.0441\n"
     ]
    }
   ],
   "source": [
    "list_relevant_features = getRelFeatures()\n",
    "df_dropped_features = drop_features(df, list_relevant_features)\n",
    "#column_list = df.columns.to_list().remove('price')\n",
    "X_train, X_test, y_train, y_test = splitData(df_dropped_features, SPLIT_SIZE)\n",
    "model = reg_train_test(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> hinweise aus der vorlesung die interessant sein könnten:\n",
    "* principal components\n",
    "* EFA – Determine number of factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Selection: Experiment with different regression algorithms, e.g. linear regression, polynomial regression, regression trees etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Hyper-parameter Tuning: Change the hyper-parameters of your algorithms (e.g.„degree“ in case of polynomial regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49d1cf207c92197553c1326cc52484d1ee2809997f5109c15474876a3e083b6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "954f5f24ad6d00adbc06a20695ec5fdb3f1a5fa43282de1749a6705a15dcbfc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
